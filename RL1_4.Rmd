# NTUTrainRL1_4
This is the lecture notes for NTU Train Course of R Project Lecture 1_4

## Data Collection

### 讀取csv

這是實務中最常運用的方法，csv as in comma-separated values。

```{r}
setwd("C:/NTUTrainRL1")
Sys.setlocale(category = "LC_ALL", locale = "cht")#csv檔是繁體中文
accidentList <- read.csv("data/funCoastAccident.csv", header=TRUE, sep=",")
head(accidentList)#看看前6筆資料
str(accidentList)#看看資料集結構
```

欄位屬性可以在讀取資料時設定。

```{r}
accidentList <- read.csv("data/funCoastAccident.csv", header=TRUE, sep=",", row.names="編號", colClasses=c("character", "character", "character", "character", "character", "integer", "factor", "factor"))
colnames(accidentList) <- c("county", "hospital", "gender", "nationality", "age", "woundType1", "woundType2")
```

### 讀取excel

我們需要仰賴xlsx套件才能夠讀取excel檔案。

```{r}
install.packages("xlsx")
library(xlsx)
accidentList <- read.xlsx("C:/NTUTrainRL1/data/funCoastAccident.xlsx", 1)
```

### 讀取sas資料集

我們需要仰賴Hmisc套件才能夠讀取sas資料集。

```{r}
library(Hmisc)
datadir <- "C:/NTUTrainRL1/data"
sasexe <- "C:/Program Files/SASHome/SASFoundation/9.4/sas.exe"
accidentList <- sas.get(libraryName=datadir, member="funcoastaccident", sasprog=sasexe)
```

### 抓取網頁資料
網頁爬蟲是一個專門的研究領域，這裡只有時間很快地簡介，有興趣的學員可以去選修專門為蟲友開的課程。一個完整的爬蟲流程通常會有以下這三個程序:

1. Connector
2. Parser
3. Database(在本單元中不涵蓋)

#### 工欲善其事
我們要先裝一些好用的網頁瀏覽器外掛來幫助我們爬網頁。
* CHROME
  * [Quick Javascript Switcher](https://chrome.google.com/webstore/detail/quick-javascript-switcher/geddoclleiomckbhadiaipdggiiccfje)
  * [XPATH Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=zh-TW)
  * [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc?hl=zh-TW)
  * [SelectorGadget](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en)
* FIREFOX
  * [Cookies Manager+](https://addons.mozilla.org/zh-tw/firefox/addon/cookies-manager-plus/)
  * [HackBar](https://addons.mozilla.org/zh-tw/firefox/addon/hackbar/)

#### Connector

網頁資料藏在哪裡? 首先在Chrome瀏覽網頁中按F12叫出Chrome開發者介面, 點選Network。
* Doc(Documents)
* XHR(XHR and Fetch)
* JS(Scripts)
* WS(Websockets)

來練習一下！

* [奇摩股市](https://tw.stock.yahoo.com/d/s/major_2330.html)
  * Doc
  * major_2330.html
* [批踢踢八卦版](https://www.ptt.cc/ask/over18?from=%2Fbbs%2FGossiping%2Findex.html)
  * Doc
  * index.html
* [PCHome購物中心](http://ecshweb.pchome.com.tw/search/v3.3/?q=sony&scope=all)
  * XHR
  * results?q=sony&page=1&sort=rnk/dc
  * Request Method:GET(較簡單可以看到資料內容)
  * http://ecshweb.pchome.com.tw/search/v3.3/all/results?q=sony&page=1&sort=rnk/dc
  * 可以看到JSON格式的資料內容
* [7-11全國分店](http://emap.pcsc.com.tw/emap.aspx)
  * XHR
  * EMapSDK.aspx
  * Request Method:POST(較複雜才可以看到資料內容)
  * 打開Firefox按F9打開開發者介面
  * URL處貼Request URL
  * Post data處貼Form Data view source
  * 可以看到XML格式的資料內容

#### Parser
我們已經知道整包資料在哪裡，但多數時候我們不需要其他雜七雜八的資料，可以使用兩種Selector方法只把我們想要的資料選取出來。
* CSS Selector
  * 使用SelectorGadget外掛
  * 按F12叫出console使用`document.querySelectorAll("CSS Selector Here")`驗證
* XPath Selector
  * 按Ctrl+Shift+X叫出XPath Helper
  * 按住Shift將滑鼠游標移至網頁欲抓取的資料處

來練習一下!

* [奇摩股市](https://tw.stock.yahoo.com/d/s/major_2330.html)
  * CSS Selector抓個股當日買超券商: td.ttt:nth-child(1)
  * XPath Selector抓個股當日買超券商: //td[@class='ttt'][1]
    
* [批踢踢八卦版](https://www.ptt.cc/ask/over18?from=%2Fbbs%2FGossiping%2Findex.html)
  * CSS Selector抓文章標題: div.title>a
  * XPath Selector抓文章標題: //div[@class='title']/a
    
* [PCHome購物中心](http://ecshweb.pchome.com.tw/search/v3.3/?q=sony&scope=all)
  * CSS Selector抓商品價格: h6>strong
  * XPath Selector抓商品價格: //h6/strong
    
* [7-11全國分店](http://emap.pcsc.com.tw/emap.aspx)
  * CSS Selector抓門市情報: td>table.mytb_1
  * XPath Selector抓門市情報: //td[@class='alt'][2]
    
#### R爬蟲工具箱

* rvest
* httr
* XML
* xml2
* RCurl

#### 東森房屋範例

```{r}
library(httr)
library(rvest)
library(stringr)

city <- "台北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s", 
              URLencode(city))

# Get the max index
max_index <- read_html(url) %>% 
    html_nodes(xpath = "//div[@class='page']/a") %>%
    html_text %>%
    str_extract("[0-9]+") %>%
    as.integer %>% 
    max(na.rm = TRUE)

# Construct all urls
urls <- sprintf("http://www.etwarm.com.tw/object_list?city=%s&page=",
               URLencode(city))
urls <- paste0(urls, 1:max_index)

# Get data
houses <- data.table()
# for (i in 1:length(urls)) {
for (i in 1:3) {
    res <- read_html(urls[i]) 
    district <- res %>% 
        html_nodes(xpath <- "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>% 
        html_text %>%
        iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Wondows  
        str_extract(sprintf("%s.*區", city))
    price <- res %>% 
        html_nodes(xpath = "//div[@class='price']") %>% 
        html_text %>% 
        str_replace_all(",", "") %>% 
        as.numeric()
    temp <- data.frame(district = district,
                      price = price)  
    houses = rbind(houses, temp)
    Sys.sleep(abs(rnorm(1)))
    print(urls[i])
}
```

#### 7-Eleven門市資訊範例

```{r}
library(httr)
library(XML)
library(stringr)

get_stores <- function(city, town) {
    res <- POST("http://emap.pcsc.com.tw/EMapSDK.aspx",
               body = list(commandid="SearchStore", 
                           city = city,
                           town = town))
    stores <- xmlParse(content(res, as = "text")) %>% 
        .["//GeoPosition"] %>% 
        xmlToDataFrame
    return(stores)
}

stores <- get_stores("台北市", "大安區")
stores
```

## Reference
* R in Action, Robert I. Kabacoff
* The Art of R Programming, Norman Matloff
* 木刻思RCrawler

&copy; Tony Yao-Jen Kuo 2015