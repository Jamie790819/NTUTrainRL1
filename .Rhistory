district <- res %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Windows
str_extract(sprintf("%s.*區", area))
res
district
etwarm <- read_html(urls[1])
district <- etwarm %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8")
district
district <- etwarm %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Windows
str_extract
district <- etwarm %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Windows
str_extract(.,"台北市*區")
district
district <- etwarm %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Windows
str_extract(.,"台北市")
district
district <- etwarm %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Windows
str_extract(.,"台北市.*區")
district
for (i in 1:3) {
etwarm <- read_html(urls[i])
district <- etwarm %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Windows
str_extract(.,sprintf("%s.*區", area))# "台北市.*區"
price <- etwarm %>%
html_nodes(xpath = "//div[@class='price']") %>%
html_text %>%
str_replace_all(",", "") %>%
as.numeric()
temp <- data.frame(district = district,
price = price)
houses = rbind(houses, temp) #每抓一頁就往下append
Sys.sleep(abs(rnorm(1))) #不要一直抓
print(urls[i])
}
View(houses)
head(houses)
get.locale()
Sys.getlocale(category = "LC_ALL")
Sys.setlocale(category = "LC_ALL", locale = "CHT")
area <- "台北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s", URLencode(area))#把URLencode(area)代入%s
# Get the max index看總共有幾頁要爬
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
# Construct all urls做出'max_index'個URLs
urls <- sprintf("http://www.etwarm.com.tw/object_list?area=%s&page=", URLencode(area))
urls <- paste0(urls, 1:max_index)#concatenate and remove blank
# Get data開始抓資料
houses <- data.frame()
# for (i in 1:length(urls)) {
# 先抓前3頁就好比較快
for (i in 1:3) {
etwarm <- read_html(urls[i])
district <- etwarm %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Windows
str_extract(.,sprintf("%s.*區", area))# "台北市.*區"
price <- etwarm %>%
html_nodes(xpath = "//div[@class='price']") %>%
html_text %>%
str_replace_all(",", "") %>%
as.numeric()
temp <- data.frame(district = district,
price = price * 10000)
houses = rbind(houses, temp) #每抓一頁就往下append
Sys.sleep(abs(rnorm(1))) #不要一直抓
print(urls[i]) #告訴user這次抓了哪幾張
}
Sys.setlocale(category = "LC_ALL", locale = "CHT")
area <- "台北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s", URLencode(area))#把URLencode(area)代入%s
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(na.rm = TRUE) #取最大值
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a")
area <- "台北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s", URLencode(area))#把URLencode(area)代入%s
url
max_index <- read_html(url)
q()
Sys.setlocale(category = "LC_ALL", locale = "cht")#csv檔是繁體中文
area <- "台北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s", URLencode(area))#把URLencode(area)代入%s
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
library(magrittr)
library(httr)
library(rvest)
library(stringr)
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
max_index <- read_html(url) %>%
max_index <- read_html(url)
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a")
max_index <- read_html(url) %>%
html_nodes(xpath = "//div[@class='page']/a")
#Sys.setlocale(category = "LC_ALL", locale = "")
Sys.setlocale(category = "LC_ALL", locale = "")
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
max_index <- read_html(url) %>%
html_nodes(xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract("[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(na.rm = TRUE) #取最大值
Sys.setlocale(category = "LC_ALL", locale = "cht")#csv檔是繁體中文
city = "台北市"
url = sprintf("http://www.etwarm.com.tw/object_list.php?city=%s",
URLencode(city))
# Get the max index
max_index = read_html(url) %>%
html_nodes(xpath = "//div[@class='page']/a") %>%
html_text %>%
str_extract("[0-9]+") %>%
as.integer %>%
max(na.rm = TRUE)
max_index <- read_html(url) %>%
html_nodes(xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract("[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(na.rm = TRUE) #取最大值
city = "台北市"
url = sprintf("http://www.etwarm.com.tw/object_list.php?city=%s",
URLencode(city))
# Get the max index
max_index = read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>%
html_text %>%
str_extract(.,"[0-9]+") %>%
as.integer %>%
max(.,na.rm = TRUE)
rm(area)
rm(city)
rm(max_index)
rm(url)
area <- "台北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s",URLencode(area))#把URLencode(area)代入%s
url
Sys.setlocale(category = "LC_ALL", locale = "cht")#csv檔是繁體中文
area <- "台北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s",URLencode(area))#把URLencode(area)代入%s
url
Sys.setlocale(category = "LC_ALL", locale = "")
area <- "台北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s",URLencode(area))#把URLencode(area)代入%s
url
rm(area)
rm(url)
Sys.setlocale(category = "LC_ALL", locale = "")
area <- "台北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s",URLencode(area))#把URLencode(area)代入%s
Sys.setlocale(category = "LC_ALL", locale = "cht")
# Get the max index看總共有幾頁要爬
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
rm(area)
rm(max_index)
rm(url)
Sys.setlocale(category = "LC_ALL", locale = "")
area <- "台北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s",URLencode(area))#把URLencode(area)代入%s
# Get the max index看總共有幾頁要爬
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
# Construct all urls做出'max_index'個URLs
urls <- sprintf("http://www.etwarm.com.tw/object_list?area=%s&page=", URLencode(area))
urls <- paste0(urls, 1:max_index)#concatenate and remove blank
Sys.setlocale(category = "LC_ALL", locale = "cht")
houses <- data.frame()
# for (i in 1:length(urls)) {
# 先抓前3頁就好比較快
for (i in 1:3) {
etwarm <- read_html(urls[i])
district <- etwarm %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Windows
str_extract(.,sprintf("%s.*區", area))# "台北市.*區"
price <- etwarm %>%
html_nodes(xpath = "//div[@class='price']") %>%
html_text %>%
str_replace_all(",", "") %>%
as.numeric()
temp <- data.frame(district = district,
price = price * 10000)
houses = rbind(houses, temp) #每抓一頁就往下append
Sys.sleep(abs(rnorm(1))) #不要一直抓
print(urls[i]) #告訴user這次抓了哪幾張
}
Sys.setlocale(category = "LC_ALL", locale = "")
area <- "台北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s",URLencode(area))#把URLencode(area)代入%s
# Get the max index看總共有幾頁要爬
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
# Construct all urls做出'max_index'個URLs
urls <- sprintf("http://www.etwarm.com.tw/object_list?area=%s&page=", URLencode(area))
urls <- paste0(urls, 1:max_index)#concatenate and remove blank
# Get data開始抓資料
houses <- data.frame()
# for (i in 1:length(urls)) {
# 先抓前3頁就好比較快
for (i in 1:3) {
etwarm <- read_html(urls[i])
district <- etwarm %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Windows
str_extract(.,sprintf("%s.*區", area))# "台北市.*區"
price <- etwarm %>%
html_nodes(xpath = "//div[@class='price']") %>%
html_text %>%
str_replace_all(",", "") %>%
as.numeric()
temp <- data.frame(district = district,
price = price * 10000)
houses = rbind(houses, temp) #每抓一頁就往下append
Sys.sleep(abs(rnorm(1))) #不要一直抓
print(urls[i]) #告訴user這次抓了哪幾張
}
Sys.setlocale(category = "LC_ALL", locale = "cht")
head(houses)
View(houses)
get_stores <- function(city, town) {
res <- POST("http://emap.pcsc.com.tw/EMapSDK.aspx",
body = list(commandid="SearchStore", city = city, town = town))
stores <- xmlParse(content(res, as = "text")) %>%
.["//GeoPosition"] %>%
xmlToDataFrame
return(stores)
}
stores <- get_stores("台北市", "大安區")
View(stores)
library(httr)
library(XML)
library(stringr)
get_stores <- function(city, town) {
res <- POST("http://emap.pcsc.com.tw/EMapSDK.aspx",
body = list(commandid="SearchStore", city = city, town = town))
stores <- xmlParse(content(res, as = "text")) %>%
.["//GeoPosition"] %>%
xmlToDataFrame
return(stores)
}
stores <- get_stores("台北市", "大安區")
View(stores)
get_stores <- function(city, town) {
pcsc <- POST("http://emap.pcsc.com.tw/EMapSDK.aspx",
body <- list(commandid="SearchStore", city = city, town = town))
stores <- xmlParse(content(res, as = "text")) %>%
.["//GeoPosition"] %>%
xmlToDataFrame
return(stores)
}
storeDaan <- get_stores("台北市", "大安區")
View(storeDaan)
get_stores <- function(city, town) {
pcsc <- POST("http://emap.pcsc.com.tw/EMapSDK.aspx",
body <- list(commandid="SearchStore", city = city, town = town))
stores <- xmlParse(content(pcsc, as = "text")) %>%
.["//GeoPosition"] %>%
xmlToDataFrame
return(stores)
}
storeDaan <- get_stores("台北市", "大安區")
pcsc <- POST("http://emap.pcsc.com.tw/EMapSDK.aspx",
body = list(commandid="SearchStore", city = city, town = town))
POST("http://emap.pcsc.com.tw/EMapSDK.aspx")
POST("http://emap.pcsc.com.tw/EMapSDK.aspx", body = list(commandid = "SearchStore"))
xmlParse(content(POST("http://emap.pcsc.com.tw/EMapSDK.aspx", body = list(commandid = "SearchStore")), as="text"))
get_stores <- function(city, town) {
pcsc <- POST("http://emap.pcsc.com.tw/EMapSDK.aspx", body = list(commandid = "SearchStore", city = city, town = town))
stores <- xmlParse(content(pcsc, as = "text")) %>%
.["//GeoPosition"] %>%
xmlToDataFrame
return(stores)
}#從Firefox開發者介面觀察
View(get_stores)
storeDaan <- get_stores("台北市", "大安區")
View(storeDaan)
q()
setwd("C:/NTUTrainRL1")
Sys.getlocale()
Sys.setlocale(category = "LC_ALL", locale = "cht")#csv檔是繁體中文
Sys.getlocale()
?read.csv
accidentList <- read.csv("data/funCoastAccident.csv", header=TRUE, sep=",")
head(accidentList)#看看前6筆資料
str(accidentList)#看看資料集結構
View(accidentList)
accidentList <- read.csv("data/funCoastAccident.csv", header=TRUE, sep=",", row.names="編號", colClasses=c("character", "character", "character", "character", "character", "integer", "factor", "factor"))
colnames(accidentList) <- c("county", "hospital", "gender", "nationality", "age", "woundType1", "woundType2")
head(accidentList)
str(accidentList)
install.packages("xlsx")
library(xlsx)
accidentList <- read.xlsx("C:/NTUTrainRL1/data/funCoastAccident.xlsx", 1)
head(accidentList)
View(accidentList)
library(Hmisc)
install.packages("Hmisc")
library(Hmisc)
datadir <- "C:/NTUTrainRL1/data"
sasexe <- "C:/Program Files/SASHome/SASFoundation/9.4/sas.exe"
rm(accidentList)
accidentList <- sas.get(libraryName=datadir, member="funcoastaccident", sasprog=sasexe)
head(accidentList)
rm(accidentList)
accidentList <- read.xlsx("C:/NTUTrainRL1/data/funCoastAccident.xlsx", 1, encoding = "big-5")#encoding issue here
head(accidentList)
rm(accidentList)
read.xlsx("C:/NTUTrainRL1/data/funCoastAccident.xlsx", 1, encoding = "UTF-8")#encoding issue here
accidentList <- read.xlsx("C:/NTUTrainRL1/data/funCoastAccident.xlsx", 1, encoding = "UTF-8")#encoding issue here
head(accidentList)
rm(accidentList)
accidentList <- read.xlsx("C:/NTUTrainRL1/data/funCoastAccident.xlsx", 1, encoding = "UTF-8")#encoding issue here
View(accidentList)
rm(accidentList)
read.xlsx("C:/NTUTrainRL1/data/funCoastAccident.xlsx", 1, encoding = "UTF-8")#encoding issue here
View(accidentList)
accidentList <- read.xlsx("C:/NTUTrainRL1/data/funCoastAccident.xlsx", 1, encoding = "UTF-8")#encoding issue here
View(accidentList)
head(accidentList)
library(magrittr)
library(httr)
library(rvest)
library(stringr)
Sys.setlocale(category = "LC_ALL", locale = "")
area <- "台北市"
print("Hello World!")
sprintf("Hello %s", 'World!')
URLencode(area)
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s",URLencode(area))
url
max_index <- read_html(url)
max_index
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a")
max_index
max_index
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text
max_index
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+")
max_index
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
urls <- sprintf("http://www.etwarm.com.tw/object_list?area=%s&page=", URLencode(area))
a <- "Hello "
b <- "World!"
paste(a, b)
a <- "Hellow"
a <- "Hello"
paste(a, b)
paste(a, b, sep='')
urls <- paste0(urls, 1:max_index)#concatenate and remove blank
head(urls, 3)
houses <- data.frame()
length(urls)
for (i in 1:3) {
etwarm <- read_html(urls[i])
district <- etwarm %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Windows
str_extract(.,sprintf("%s.*區", area))# "台北市.*區"
price <- etwarm %>%
html_nodes(xpath = "//div[@class='price']") %>%
html_text %>%
str_replace_all(",", "") %>%
as.numeric()
temp <- data.frame(districtDF = district, priceDF = price * 10000)#還原單位
houses = rbind(houses, temp) #每抓一頁就往下append
Sys.sleep(abs(rnorm(1))) #不要一直抓
print(urls[i]) #告訴user這次抓了哪幾頁
}
View(houses)
head(houses)
Sys.setlocale(category = "LC_ALL", locale = "cht")
View(houses)
library(httr)
library(XML)
library(stringr)
get_stores <- function(city, town) {
pcsc <- POST("http://emap.pcsc.com.tw/EMapSDK.aspx", body = list(commandid = "SearchStore", city = city, town = town))
stores <- xmlParse(content(pcsc, as = "text")) %>%
.["//GeoPosition"] %>%
xmlToDataFrame
return(stores)
}#從Firefox開發者介面觀察
storeDaan <- get_stores("台北市", "大安區")
View(storeDaan)
Sys.setlocale(category = "LC_ALL", locale = "")
area <- "新北市"
url <- sprintf("http://www.etwarm.com.tw/object_list.php?area=%s",URLencode(area))#把URLencode(area)代入%s
# Get the max index看總共有幾頁要爬
max_index <- read_html(url) %>%
html_nodes(.,xpath = "//div[@class='page']/a") %>% #使用xpath helper
html_text %>% #Extract contents inside tags取標籤中的內容
str_extract(.,"[0-9]+") %>% #使用Regular expression把數字抓出來
as.integer %>% #化成整數
max(.,na.rm = TRUE) #取最大值
# Construct all urls做出'max_index'個URLs
urls <- sprintf("http://www.etwarm.com.tw/object_list?area=%s&page=", URLencode(area))
urls <- paste0(urls, 1:max_index)#concatenate and remove blank
# Get data開始抓資料
houses <- data.frame()
# for (i in 1:max_index) {
# 先抓前3頁就好比較快
for (i in 1:3) {
etwarm <- read_html(urls[i])
district <- etwarm %>%
html_nodes(.,xpath = "//li[@class='obj_item']/div[@class='obj_info']/h3/a") %>%
html_text %>%
iconv(from = "UTF-8", to = "UTF-8") %>% # to compatible in Windows
str_extract(.,sprintf("%s.*區", area))# "台北市.*區"
price <- etwarm %>%
html_nodes(xpath = "//div[@class='price']") %>%
html_text %>%
str_replace_all(",", "") %>%
as.numeric()
temp <- data.frame(districtDF = district, priceDF = price * 10000)#還原單位
houses = rbind(houses, temp) #每抓一頁就往下append
Sys.sleep(abs(rnorm(1))) #不要一直抓
print(urls[i]) #告訴user這次抓了哪幾頁
}
Sys.setlocale(category = "LC_ALL", locale = "cht")
View(houses)
()
q()
